#!/usr/bin/python3

import os
import sys
import json
import argparse
import numpy as np
import math
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import csv

def parse_iftop_bytes_with_unit(amount):
    if 'GB' in amount:
        amount = amount.strip('GB')
        return float(amount)
    if 'MB' in amount:
        amount = amount.strip('MB')
        return float(amount) / 1000
    if 'KB' in amount:
        amount = amount.strip('KB')
        return float(amount) / 1000 ** 2
    if 'B' in amount:
        amount = amount.strip('B')
        return float(amount) / 1000 ** 3 
    print("Unknown amount: {}".format(amount))

def parse_iftop_log(iftop_file, machine_file, exp_duration, result_file):
    gb_sent = 0.0
    gb_received = 0.0
    ips = []
    with open(machine_file) as f:
        config = json.load(f)
        for machine_info in config['machines'].values():
            ips.append(machine_info['ip'])
    lines = open(iftop_file, 'r').readlines()
    for line in lines:
        chunks = line.split()
        for ip in ips:
            if ip in chunks:
                arrow_index = 0
                if '=>' in chunks:
                    arrow_index = chunks.index('=>')
                    cumulative_index = arrow_index + 4
                    gb_sent += parse_iftop_bytes_with_unit(chunks[cumulative_index])
                else:
                    arrow_index = chunks.index('<=')
                    cumulative_index = arrow_index + 4
                    gb_received += parse_iftop_bytes_with_unit(chunks[cumulative_index])

    gb_sent = round(gb_sent, 4)
    gb_received = round(gb_received, 4)

    write_header = not os.path.exists(result_file)
    with open(result_file, 'a', encoding='UTF8', newline='') as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(['gb_sent', 'gb_received', 'mb_per_second_sent', 'mb_per_second_received'])
        data = [
            gb_sent,
            gb_received,
            round(gb_sent * 1000 * 8 / exp_duration, 2),
            round(gb_received * 1000 * 8 / exp_duration, 2),
        ]
        writer.writerow(data)


def workload_info(workload):
    if workload.lower() == 'empty-tag':
        return 'only the empty tag'
    elif workload.lower() == 'one-tag-only':
        return 'always the same tag'
    elif workload.lower() == 'new-tags-always':
        return 'always a new tag'

def concatenate_csv_files(directory, filter, result_file):
    csv_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv') and filter in file]
    with open(result_file, 'w') as f:
        for csv_file in csv_files:
            with open(csv_file) as f_in:
                for line in f_in:
                    f.write(line)

def discard_csv_files(directory, ts_end):
    csv_file_names = [file for file in os.listdir(directory) if file.endswith('.csv')]
    csv_files_to_discard = []
    for csv_file_name in csv_file_names:
        chunks = csv_file_name.split('.')[0].split('-')
        ts = int(chunks[3])
        if ts_end < ts:
            csv_files_to_discard.append(os.path.join(directory, csv_file_name))
    for csv_file in csv_files_to_discard:
        os.remove(csv_file)

def discard_csv_entries(csv_file, ts, before = True):
    df = pd.read_csv(csv_file)
    if before:
        df.drop(df[df.ts_absolute < ts].index, inplace=True)
    else:
        df.drop(df[ts < df.ts_absolute].index, inplace=True)
    df.to_csv((csv_file), mode='w', sep=',', index=False, header=True)

def merge_csv_files(directory, filter, result_file):
    csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and filter in file]
    print("Merge {}".format(', '.join(csv_files)))
    combined_csv = pd.concat([pd.read_csv(os.path.join(directory, file)) for file in csv_files])
    combined_csv.to_csv(result_file, index=False)

def count_csv_entries(csv_files):
    return pd.concat([pd.read_csv(file) for file in csv_files]).shape[0]

def count_csv_entries_in_directory(directory, filter):
    csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and filter in file]
    csv_entries = 0
    for file in csv_files:
        with open(os.path.join(directory, file)) as f:
            for line in f:
                csv_entries += 1
    return csv_entries

def large_file_compute_column_average(file, index):
    csv_entries = 0
    total = 0.0
    with open(file) as f:
        for line in f:
            chunks = line.split(',')
            csv_entries += 1
            total += float(chunks[index])
    print(round(total / csv_entries, 2))

def compute_throughput(directory, exp_duration):
    print(round(count_csv_entries_in_directory(directory, '') / exp_duration / 1000, 2))

def compute_csv_percentile(csv_file, column_ix, percentile):
    return pd.read_csv(csv_file).iloc[:, column_ix].quantile(q=percentile)

def compute_column_average(csv_file, column):
    print(pd.read_csv(csv_file)[column].mean())

def create_operation_statistics(directory, slog, result_directory):
    csv_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv') and "op-stat-" in file]
    columns = 7 # convention
    data = columns * [0]
    for csv_file in csv_files:
        with open(csv_file, newline='') as f:
            records_row = next(csv.reader(f))
            for i in range(len(data)):
                data[i] += int(records_row[i])
    with open(os.path.join(result_directory, 'operations.csv'), 'w', encoding='UTF8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([
            'slog',
            'append_ops', 'read_ops', 
            'local_index_hits', 'local_index_misses',
            'log_cache_hits', 'log_cache_misses',
            'index_min_read_ops',
        ])
        writer.writerow(
            [slog] + data
        )
    local_index_hit_ratio = round(int(data[2]) / int(data[1]), 2)
    log_cache_hit_ratio = round(int(data[4]) / int(data[1]), 2)
    ratio_text = '\n'.join((
        r'Local Index Hit Ratio: {}'.format(str(local_index_hit_ratio)),
        r'Log Cache Hit Ratio: {}'.format(str(log_cache_hit_ratio))
    ))
    with open(os.path.join(result_directory, 'operations-ratio.txt'), 'w', encoding='UTF8', newline='') as f:
        f.write(ratio_text)

def generate_operation_statistics_plot(op_stat_csv_file, result_file):
    op_stat_header_row = None
    op_stat_data_row = None
    with open(op_stat_csv_file, newline='') as f:
        reader = csv.reader(f)
        op_stat_header_row = next(reader)
        op_stat_data_row = next(reader)

    slog = op_stat_data_row[0]

    # remove slog
    op_stat_header_row = op_stat_header_row[1:]
    op_stat_data_row = op_stat_data_row[1:]

    # remove index min read ops
    op_stat_header_row = op_stat_header_row[:-1]
    op_stat_data_row = op_stat_data_row[:-1]

    op_stat_pretty_header_row = ['Appends', 'Reads', 'Index hits', 'Index misses', 'Record cache hits', 'Record cache misses']
    op_stat_display_row = [0] * len(op_stat_data_row)
    # as multiplied by 10**3
    for i in range(len(op_stat_data_row)):
        op_stat_display_row[i] = round((int(op_stat_data_row[i]) / 10**3),1)

    fig, ax = plt.subplots(1,1)

    color = ''
    if 'indilog' in slog.lower():
        color = 'darkgreen'
    if 'indilog-remote' in slog.lower():
        color = 'orange'
    if 'indilog-small-index' in slog.lower():
        color = 'forestgreen'
    if 'indilog-min-completion' in slog.lower():
        color = 'darkorange'
    if 'boki' in slog.lower():
        color = 'royalblue'
    if 'boki-local' in slog.lower():
        color = 'royalblue'
    if 'boki-remote' in slog.lower():
        color = 'darkviolet'
    if 'boki-hybrid' in slog.lower():
        color = 'grey'

    ax.grid(color='#95a5a6', linestyle='--', linewidth=1, axis='y', alpha=0.7)
    # scale_y = 1e3
    # ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/scale_y))
    # ax.yaxis.set_major_formatter(ticks_y)
    ax.set_ylabel('Counts in thousands', fontsize=12)
    ind = np.arange(len(op_stat_pretty_header_row))
    ax.set_xticks(ind, labels=op_stat_pretty_header_row, fontsize=12, rotation=22)
    ax.set_ylim(top=500)

    bars = ax.bar(range(len(op_stat_display_row)), op_stat_display_row, color=color)
    for bars in ax.containers:
        ax.bar_label(bars, fontsize=12)

    # local_index_hit_ratio = round(int(op_stat_data_row[2]) / int(op_stat_data_row[1]), 2)
    # log_cache_hit_ratio = round(int(op_stat_data_row[4]) / int(op_stat_data_row[1]), 2)

    # info_text = '\n'.join((
    #     r'Local Index Hit Ratio: {}'.format(str(local_index_hit_ratio)),
    #     r'Log Cache Hit Ratio: {}'.format(str(log_cache_hit_ratio))
    # ))
    # ax.text(
    #     0.80, 1.15, info_text,
    #     transform=ax.transAxes, verticalalignment='top', size=8,
    #     bbox=dict(
    #         boxstyle='round', facecolor='wheat', alpha=0.5
    #     )
    # )
    plt.savefig(result_file, bbox_inches='tight')
    plt.close

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('cmd', type=str)
    parser.add_argument('--directory', type=str)
    parser.add_argument('--file', type=str)
    parser.add_argument('--machine-file', type=str)
    parser.add_argument('--slog', type=str)
    parser.add_argument('--exp-duration', type=int)
    parser.add_argument('--interval', type=int)
    parser.add_argument('--reference-ts', type=int)
    parser.add_argument('--filter', type=str)
    parser.add_argument('--ts-end', type=int)
    parser.add_argument('--ts', type=int)
    parser.add_argument('--column', type=str)
    parser.add_argument('--divisor', type=int)
    parser.add_argument('--index', type=int)
    parser.add_argument('--round-decimals', type=int)
    parser.add_argument('--workload', type=str)
    parser.add_argument('--result-file', type=str)
    parser.add_argument('--result-directory', type=str)
    args = parser.parse_args()
    try:
        if args.cmd == 'merge-csv':
            merge_csv_files(args.directory, args.filter, args.result_file)
        elif args.cmd == 'concatenate-csv':
            concatenate_csv_files(args.directory, args.filter, args.result_file)
        elif args.cmd == 'discard-csv-files':
            discard_csv_files(args.directory, args.ts_end)
        elif args.cmd == 'discard-csv-entries-before':
            discard_csv_entries(args.file, args.ts, before=True)
        elif args.cmd == 'discard-csv-entries-after':
            discard_csv_entries(args.file, args.ts, before=False) # == after
        elif args.cmd == 'compute-throughput':
            compute_throughput(args.directory, args.exp_duration)
        elif args.cmd == 'add-slog-info':
            add_column(args.file, 0, 'slog', args.slog)
        elif args.cmd == 'compute-column-average':
            compute_column_average(args.file, args.column)
        elif args.cmd == 'large-file-compute-column-average':
            large_file_compute_column_average(args.file, args.index)
        elif args.cmd == 'parse-iftop-log':
            parse_iftop_log(args.file, args.machine_file, args.exp_duration, args.result_file)
        elif args.cmd == 'create-operation-statistics':
            create_operation_statistics(args.directory, args.slog, args.result_directory)
        elif args.cmd == 'generate-operation-statistics-plot':
            generate_operation_statistics_plot(args.file, args.result_file)
        else:
            raise Exception('Unknown command: ' + args.cmd)
    except Exception as e:
        err_str = str(e)
        if not err_str.endswith('\n'):
            err_str += '\n'
        sys.stderr.write(err_str)
        sys.exit(1)