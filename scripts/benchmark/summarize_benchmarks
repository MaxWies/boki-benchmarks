#!/usr/bin/python3
from datetime import datetime, time
from operator import itemgetter
import os
import sys
import json
import argparse
import subprocess as sp
import matplotlib.pyplot as plt
import matplotlib.offsetbox as offsetbox
import numpy as np
import math
import pandas as pd
import csv

def present_latencies_on_timeline(directory, op, items, type, calls_success, exp_duration_sec, counter):
    latencies = [i['latency']/10**3 for i in items]
    events = [i['relative_timestamp']/10**6 for i in items]
    plt.plot(events, latencies, 'rx', label=op)
    plt.title('Latencies-'+type+ ':' + ' {} from {} successful operation calls'.format(len(items), calls_success))
    plt.xlabel('Relative experiment time (s)')
    plt.xlim([0, exp_duration_sec])
    plt.ylabel('Latency (ms)')
    plt.legend(loc='upper left')
    plt.savefig(os.path.join(directory, 'latencies-'+type+' '+str(counter)+'.png'))
    plt.close()

def present_tail_latencies_on_timeline(directory, benchmark):
    for i in range(len(benchmark['operations'])):
        operation = benchmark['operations'][i]
        present_latencies_on_timeline(directory, operation['description'], operation["latency_tail"]["items"], 'tail', operation["calls_success"], benchmark['time_log']['loop_duration'], i)

def get_color_from_operation(operation):
    if operation['description'].lower() == 'append':
        return 'b'
    if operation['description'].lower() == 'read':
        return 'g'
    return 'r'

def present_operation_latencies_on_cdf(directory, benchmark, counter):
    cdf = []
    latencies = []
    calls_success_until = 0
    highest_slot_with_entry = 0
    operation = benchmark['operations'][counter]
    bucket = operation['bucket']
    _, ax = plt.subplots()
    #probability = 0
    #granularity = 0.001

    for i in range(len(bucket['slots'])):
        if 0.999 < calls_success_until / operation['calls_success']:
            break
        for j in range(bucket['slots'][i]):
            latencies.append(bucket['lower'] + i * bucket['interval'])
            calls_success_until += 1 #bucket['slots'][i]
            cdf.append(calls_success_until/operation['calls_success'])
        if 0 < bucket['slots'][i]:
            highest_slot_with_entry = i
    plt.plot(latencies, cdf, get_color_from_operation(operation), label=operation['description'])

    percentiles = [0.5, 0.75, 0.9, 0.99, 0.999]
    latencies_percentiles = [operation["latency_p50"],operation["latency_p75"],operation["latency_p90"],operation["latency_p99"],operation["latency_p99.9"]]
    plt.plot(latencies_percentiles, percentiles, get_color_from_operation(operation) + 'o')
    for i, v in enumerate(latencies_percentiles):
        plt.annotate('  ' + str(v) + '\u03BCs' + ' ({})'.format(str(percentiles[i])), (latencies_percentiles[i], percentiles[i]*0.97))

    #plt.text(0.95, 0.01, 'latency min: %{}'.format(operation['latency_min']), verticalalignment='bottom', horizontalalignment='right', color='green', fontsize=15)
    at = offsetbox.AnchoredText('calls: {}\ncalls success: {}\nlatency min: {}\u03BCs\nlatency max: {}\u03BCs'.format(operation['calls'], operation['calls_success'], operation['latency_min'], operation['latency_max']), loc='lower right')
    ax.add_artist(at)

    upperXlim = bucket['lower'] + highest_slot_with_entry * bucket['interval']
    upperXlim *= 1.2
    plt.xlabel('Latency (\u03BCs)')
    plt.xlim([bucket['lower'], bucket['lower'] + upperXlim])
    plt.ylim([0,1.05])

    plt.legend(loc='upper left')
    plt.title('Cumulative latency (\u03BCs)')# of {} operation'.format(operation['description']))
    plt.subplots_adjust(bottom=0.25)
    plt.figtext(0.5, 0.05,
    'Duration: {}s\nSetup: Eng{}-St{}-Seq{}\nConcurrency per engine:{}'
    .format(benchmark['time_log']['loop_duration'],
    benchmark['description']['engine_nodes'], 
    benchmark['description']['storage_nodes'], 
    benchmark['description']['sequencer_nodes'],
    benchmark['description']['concurrency_workers']),
    ha="center", va="center")

    plt.savefig(os.path.join(directory, 'latencies-cdf-{}.png'.format(str(counter))))
    plt.close()

def present_latencies_on_cdf(directory, benchmark):
    for i in range(len(benchmark['operations'])):
        present_operation_latencies_on_cdf(directory, benchmark, i)

def compute_bucket_percentile(bucket, number_of_entries, percentile):
    bucket_slot_index = 0
    threshold = percentile * number_of_entries
    entries_visited = bucket["slots"][bucket_slot_index]
    while entries_visited < threshold:
        bucket_slot_index += 1
        entries_visited += bucket["slots"][bucket_slot_index]

    return bucket["lower"] + bucket_slot_index * bucket["interval"]

def unix_timestamp_to_datetime(unix_timestamp_nanosec):
    dt = datetime.utcfromtimestamp(unix_timestamp_nanosec // 1000000000)
    nanosec = (unix_timestamp_nanosec - (unix_timestamp_nanosec // 1000000000) * 1000000000)
    microsec = nanosec // 1000
    return dt.strftime('%Y-%m-%d %H:%M:%S') + ' {}'.format(microsec)

def make_timestamps_human_readable(dic, key_matcher):
    for key in dic.keys():
        if key_matcher in key:
            dic[key] = unix_timestamp_to_datetime(dic[key])

def sort_operation_item_calls(items):
    return sorted(items, key=itemgetter('relative_timestamp'))


def generate_benchmark_log_loop_result(result_file, output_directory):
    file_name = os.path.basename(result_file)
    file_name_extensionless = file_name.split('.')[0]
    long_benchmark_result = None
    short_benchmark_result = None
    with open(result_file) as f:
        benchmark = json.load(f)
        for operation in benchmark["operations"]:
            if "bucket" in operation:
                operation["latency_p10"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.1)
                operation["latency_p50"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.5)
                operation["latency_p75"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.75)
                operation["latency_p90"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.9)
                operation["latency_p99"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.99)
                operation["latency_p99.9"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.999)
            if "latency_head" in operation and 0 < len(operation["latency_head"]["items"]):
                operation["latency_min"] = min([i["latency"] for i in operation["latency_head"]["items"]])
                operation["latency_head"]["items"] = sort_operation_item_calls(operation["latency_head"]["items"])
            if "latency_tail" in operation and 0 < len(operation["latency_tail"]["items"]):
                operation["latency_max"] = max([i["latency"] for i in operation["latency_tail"]["items"]])
                operation["latency_tail"]["items"] = sort_operation_item_calls(operation["latency_tail"]["items"])
        if "time_log" in benchmark:
            make_timestamps_human_readable(benchmark["time_log"], 'time')
        present_tail_latencies_on_timeline(output_directory, benchmark)
        present_latencies_on_cdf(output_directory, benchmark)
        for operation in benchmark["operations"]:
            del operation["bucket"]
        long_benchmark_result = json.dumps(benchmark, sort_keys=True, indent=4)
        for operation in benchmark["operations"]:
            del operation['latency_head']
            del operation['latency_tail']
        short_benchmark_result = json.dumps(benchmark, sort_keys=True, indent=4)
    if long_benchmark_result is None or short_benchmark_result is None:
        raise Exception
    with open(os.path.join(output_directory, 'long-'+file_name_extensionless+'.json'), 'w') as f:
        f.write(long_benchmark_result)
    with open(os.path.join(output_directory, 'short-'+file_name_extensionless+'.json'), 'w') as f:
        f.write(short_benchmark_result)

def generate_benchmark_log_loop_results(directory):
    output_directory = os.path.join(directory, 'output')
    if not os.path.isdir(output_directory):
        os.mkdir(output_directory)
    for f in os.listdir(directory):
        file = os.path.join(directory, f)
        if not os.path.isfile(file):
            continue
        generate_benchmark_log_loop_result(file, output_directory)

def merge_engine_stats(output_directory, counter, files):
    stat_merged = None
    for i in range(len(files)):
        stat_next = None
        with open(files[i]) as f:
            stat_next = json.load(f)
        if stat_merged == None:
            stat_merged = stat_next
            continue
        for main_key in stat_merged.keys():
            for key in stat_merged[main_key]:
                stat_merged[main_key][key] += stat_next[main_key][key]
    stat_merged_result = json.dumps(stat_merged, indent=4)
    with open(os.path.join(output_directory, 'engines-merged-'+str(counter)+'.json'), 'w') as f:
        f.write(stat_merged_result)

def generate_engine_stats(directory):
    per_engine_stat_files = {}
    for file_name in sorted(os.listdir(directory)):
        engine_id = file_name.split('-')[1]
        if engine_id not in per_engine_stat_files.keys():
            per_engine_stat_files[engine_id] = []
        per_engine_stat_files[engine_id].append(os.path.join(directory, file_name))
    per_counter_stat_files = {}
    counter = len(per_engine_stat_files[list(per_engine_stat_files.keys())[0]])
    for c in range(counter):
        if c not in per_counter_stat_files.keys():
            per_counter_stat_files[c] = []
        for key in per_engine_stat_files.keys():
            per_counter_stat_files[c].append(list(per_engine_stat_files[key])[c])
    for c in range(counter):
        merge_engine_stats(directory, c, per_counter_stat_files[c])    

def merge_csv_files(directory, filter, result_file):
    csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and filter in file]
    combined_csv = pd.concat([pd.read_csv(file) for file in csv_files])
    combined_csv.to_csv(result_file, index=False)

def count_csv_entries(directory, filter):
    csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and filter in file]
    return pd.concat([pd.read_csv(file) for file in csv_files]).shape[0]

def compute_csv_percentile(csv_file, percentile):
    df = pd.read_csv(csv_file)
    return df.quantile(
        q=percentile,
        axis=0,
        numeric_only=True,
        interpolation='linear'
    )

def add_throughput_latency_row(directory, slog_config, exp_duration, result_file):
    write_header = not os.path.exists(result_file)
    with open(result_file, 'w', encoding='UTF8', newline='') as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(['slog_config', 'throughput(kOp/s)', 'latency-append-50(ms)','latency-append-99(ms)', 'latency-read-50(ms)','latency-read-99(ms)'])
        data = [
            slog_config,
            round(count_csv_entries(directory, '') / exp_duration / 1000, 2),
            compute_csv_percentile(os.path.join(directory, "latencies-append-{}.csv".format(slog_config)), 0.5),
            compute_csv_percentile(os.path.join(directory, "latencies-append-{}.csv".format(slog_config)), 0.99),
            compute_csv_percentile(os.path.join(directory, "latencies-read-{}.csv".format(slog_config)), 0.5),
            compute_csv_percentile(os.path.join(directory, "latencies-reead-{}.csv".format(slog_config)), 0.99)
        ]
        writer.writerow(data)


def compute_throughput_from_csv_files(directory, filter, result_file):
    pass

def throughput_vs_latency(directory, result_directory):
    #systems = [name for name in os.listdir(directory) if os.path.isdir(name) and name != "."]
    print("Until here")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('cmd', type=str)
    parser.add_argument('--directory', type=str)
    parser.add_argument('--file', type=str)
    parser.add_argument('--slog-config', type=str)
    parser.add_argument('--exp-duration', type=int)
    parser.add_argument('--filter', type=str)
    parser.add_argument('--result-file', type=str)
    parser.add_argument('--result-directory', type=str)
    args = parser.parse_args()
    try:
        if args.cmd == 'benchmark-log-loop':
            generate_benchmark_log_loop_results(args.directory)
        elif args.cmd == 'benchmark-engine-stats':
            generate_engine_stats(args.directory)
        elif args.cmd == 'merge-csv':
            merge_csv_files(args.directory, args.filter, args.result_file)
        elif args.cmd == 'add-throughput-latency-row':
            add_throughput_latency_row(args.directory, args.slog_config, args.exp_duration, args.result_file)
        elif args.cmd == 'throughput-vs-latency':
            throughput_vs_latency(args.file, args.result_directory)
        else:
            raise Exception('Unknown command: ' + args.cmd)
    except Exception as e:
        err_str = str(e)
        if not err_str.endswith('\n'):
            err_str += '\n'
        sys.stderr.write(err_str)
        sys.exit(1)