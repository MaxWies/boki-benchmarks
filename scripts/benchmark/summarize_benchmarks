#!/usr/bin/python3
from datetime import datetime, time
from operator import itemgetter
import os
import sys
import json
import argparse
import subprocess as sp
import matplotlib.pyplot as plt
import matplotlib.offsetbox as offsetbox
import numpy as np
import math

def present_latencies_on_timeline(directory, op, items, type, calls_success, exp_duration_sec, counter):
    latencies = [i['latency']/10**3 for i in items]
    events = [i['relative_timestamp']/10**6 for i in items]
    plt.plot(events, latencies, 'rx', label=op)
    plt.title('Latencies-'+type+ ':' + ' {} from {} successful operation calls'.format(len(items), calls_success))
    plt.xlabel('Relative experiment time (s)')
    plt.xlim([0, exp_duration_sec])
    plt.ylabel('Latency (ms)')
    plt.legend(loc='upper left')
    plt.savefig(os.path.join(directory, 'latencies-'+type+' '+str(counter)+'.png'))
    plt.close()

def present_tail_latencies_on_timeline(directory, benchmark):
    for i in range(len(benchmark['operations'])):
        operation = benchmark['operations'][i]
        present_latencies_on_timeline(directory, operation['description'], operation["latency_tail"]["items"], 'tail', operation["calls_success"], benchmark['time_log']['loop_duration'], i)

def get_color_from_operation(operation):
    if operation['description'].lower() == 'append':
        return 'b'
    if operation['description'].lower() == 'read':
        return 'g'
    return 'r'

def present_operation_latencies_on_cdf(directory, benchmark, counter):
    cdf = []
    latencies = []
    calls_success_until = 0
    highest_slot_with_entry = 0
    operation = benchmark['operations'][counter]
    bucket = operation['bucket']
    _, ax = plt.subplots()
    #probability = 0
    #granularity = 0.001

    for i in range(len(bucket['slots'])):
        if 0.999 < calls_success_until / operation['calls_success']:
            break
        for j in range(bucket['slots'][i]):
            latencies.append(bucket['lower'] + i * bucket['interval'])
            calls_success_until += 1 #bucket['slots'][i]
            cdf.append(calls_success_until/operation['calls_success'])
        if 0 < bucket['slots'][i]:
            highest_slot_with_entry = i
    plt.plot(latencies, cdf, get_color_from_operation(operation), label=operation['description'])

    percentiles = [0.5, 0.75, 0.9, 0.99, 0.999]
    latencies_percentiles = [operation["latency_p50"],operation["latency_p75"],operation["latency_p90"],operation["latency_p99"],operation["latency_p99.9"]]
    plt.plot(latencies_percentiles, percentiles, get_color_from_operation(operation) + 'o')
    for i, v in enumerate(latencies_percentiles):
        plt.annotate('  ' + str(v) + '\u03BCs' + ' ({})'.format(str(percentiles[i])), (latencies_percentiles[i], percentiles[i]*0.97))

    #plt.text(0.95, 0.01, 'latency min: %{}'.format(operation['latency_min']), verticalalignment='bottom', horizontalalignment='right', color='green', fontsize=15)
    at = offsetbox.AnchoredText('calls: {}\ncalls success: {}\nlatency min: {}\u03BCs\nlatency max: {}\u03BCs'.format(operation['calls'], operation['calls_success'], operation['latency_min'], operation['latency_max']), loc='lower right')
    ax.add_artist(at)

    upperXlim = bucket['lower'] + highest_slot_with_entry * bucket['interval']
    upperXlim *= 1.2
    plt.xlabel('Latency (\u03BCs)')
    plt.xlim([bucket['lower'], bucket['lower'] + upperXlim])
    plt.ylim([0,1.05])

    plt.legend(loc='upper left')
    plt.title('Cumulative latency (\u03BCs)')# of {} operation'.format(operation['description']))
    plt.subplots_adjust(bottom=0.25)
    plt.figtext(0.5, 0.05,
    'Duration: {}s\nSetup: Eng{}-St{}-Seq{}\nConcurrency per engine:{}'
    .format(benchmark['time_log']['loop_duration'],
    benchmark['description']['engine_nodes'], 
    benchmark['description']['storage_nodes'], 
    benchmark['description']['sequencer_nodes'],
    benchmark['description']['concurrency_workers']),
    ha="center", va="center")

    plt.savefig(os.path.join(directory, 'latencies-cdf-{}.png'.format(str(counter))))
    plt.close()

def present_latencies_on_cdf(directory, benchmark):
    for i in range(len(benchmark['operations'])):
        present_operation_latencies_on_cdf(directory, benchmark, i)

def compute_bucket_percentile(bucket, number_of_entries, percentile):
    bucket_slot_index = 0
    threshold = percentile * number_of_entries
    entries_visited = bucket["slots"][bucket_slot_index]
    while entries_visited < threshold:
        bucket_slot_index += 1
        entries_visited += bucket["slots"][bucket_slot_index]

    return bucket["lower"] + bucket_slot_index * bucket["interval"]

def unix_timestamp_to_datetime(unix_timestamp_nanosec):
    dt = datetime.utcfromtimestamp(unix_timestamp_nanosec // 1000000000)
    nanosec = (unix_timestamp_nanosec - (unix_timestamp_nanosec // 1000000000) * 1000000000)
    microsec = nanosec // 1000
    return dt.strftime('%Y-%m-%d %H:%M:%S') + ' {}'.format(microsec)

def make_timestamps_human_readable(dic, key_matcher):
    for key in dic.keys():
        if key_matcher in key:
            dic[key] = unix_timestamp_to_datetime(dic[key])

def sort_operation_item_calls(items):
    return sorted(items, key=itemgetter('relative_timestamp'))


def show_benchmark_results(result_file, output_directory):
    file_name = os.path.basename(result_file)
    file_name_extensionless = file_name.split('.')[0]
    long_benchmark_result = None
    short_benchmark_result = None
    with open(result_file) as f:
        benchmark = json.load(f)
        for operation in benchmark["operations"]:
            if "bucket" in operation:
                operation["latency_p10"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.1)
                operation["latency_p50"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.5)
                operation["latency_p75"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.75)
                operation["latency_p90"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.9)
                operation["latency_p99"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.99)
                operation["latency_p99.9"] = compute_bucket_percentile(operation["bucket"], operation['calls_success'], 0.999)
            if "latency_head" in operation and 0 < len(operation["latency_head"]["items"]):
                operation["latency_min"] = min([i["latency"] for i in operation["latency_head"]["items"]])
                operation["latency_head"]["items"] = sort_operation_item_calls(operation["latency_head"]["items"])
            if "latency_tail" in operation and 0 < len(operation["latency_tail"]["items"]):
                operation["latency_max"] = max([i["latency"] for i in operation["latency_tail"]["items"]])
                operation["latency_tail"]["items"] = sort_operation_item_calls(operation["latency_tail"]["items"])
        if "time_log" in benchmark:
            make_timestamps_human_readable(benchmark["time_log"], 'time')
        present_tail_latencies_on_timeline(output_directory, benchmark)
        present_latencies_on_cdf(output_directory, benchmark)
        for operation in benchmark["operations"]:
            del operation["bucket"]
        long_benchmark_result = json.dumps(benchmark, sort_keys=True, indent=4)
        for operation in benchmark["operations"]:
            del operation['latency_head']
            del operation['latency_tail']
        short_benchmark_result = json.dumps(benchmark, sort_keys=True, indent=4)
    if long_benchmark_result is None or short_benchmark_result is None:
        raise Exception
    with open(os.path.join(output_directory, 'long-'+file_name_extensionless+'.json'), 'w') as f:
        f.write(long_benchmark_result)
    with open(os.path.join(output_directory, 'short-'+file_name_extensionless+'.json'), 'w') as f:
        f.write(short_benchmark_result)

def show_all_benchmark_results(directory):
    output_directory = os.path.join(directory, 'output')
    if not os.path.isdir(output_directory):
        os.mkdir(output_directory)
    for f in os.listdir(directory):
        file = os.path.join(directory, f)
        if not os.path.isfile(file):
            continue
        show_benchmark_results(file, output_directory)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--result-directory', type=str)
    args = parser.parse_args()
    try:
        show_all_benchmark_results(args.result_directory)
    except Exception as e:
        err_str = str(e)
        if not err_str.endswith('\n'):
            err_str += '\n'
        sys.stderr.write(err_str)
        sys.exit(1)