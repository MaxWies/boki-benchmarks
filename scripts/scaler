#!/usr/bin/python3

from datetime import timedelta
import os
import sys
import json
from typing import List
import yaml
import argparse
import subprocess as sp
import time

# TODO: improve
USER_NAME = 'mrc'
SWARM_MANAGER_PORT = 2377

def run_command(cmd):
    print(cmd, file=sys.stderr) # print to stderr for debugging
    ret = sp.run(cmd, stdout=sp.PIPE, stderr=sp.PIPE, encoding='utf8')
    if ret.returncode != 0:
        raise Exception('Failed to run remote command: ' + ' '.join(cmd) + '\n' + ret.stderr)
    return ret.stdout, ret.stderr

def run_remote_command(ssh_ip, cmd):
    print(cmd, file=sys.stderr) # print to stderr for debugging
    ret = sp.run(['ssh', '-q', ssh_ip + '-' + USER_NAME, '--'] + cmd,
                 stdout=sp.PIPE, stderr=sp.PIPE, encoding='utf8')
    if ret.returncode != 0:
        raise Exception('Failed to run remote command: ' + ' '.join(cmd) + '\n' + ret.stderr)
    return ret.stdout, ret.stderr

def docker_swarm_get_worker_token(ip):
    join_token, _ = run_remote_command(ip, ['docker', 'swarm', 'join-token', '-q', 'worker'])
    return join_token.strip()

def docker_swarm_join(ip, token, swarm_manager_ip):
    return run_remote_command(ip, ['docker', 'swarm', 'join', '--token', token, '{}:{}'.format(swarm_manager_ip, str(SWARM_MANAGER_PORT))])

def docker_swarm_leave(ip):
    return run_remote_command(ip, ['docker', 'swarm', 'leave'])

def docker_swarm_remove_node(ip, node_id):
    return run_remote_command(ip, ['docker', 'node', 'rm', node_id])

def docker_swarm_get_nodes_hostnames(ip):
    hostnames, _ = run_remote_command(ip, ['docker', 'node', 'ls', '--format', '{{.Hostname}}'])
    return hostnames.split('\n')

def docker_swarm_get_node_id_by_hostname(ip, hostname):
    node, _ = run_remote_command(ip, 
        ['docker', 'node', 'ls', '-f', 'name={}'.format(hostname), '--format', '"{{.ID}}"']
    )
    return node.strip()

def docker_swarm_add_label(ip, target_hostname, label):
    return run_remote_command(ip, ['docker', 'node', 'update', '--label-add', label, target_hostname])

def docker_swarm_rm_label(ip, target_hostname, label):
    return run_remote_command(ip, ['docker', 'node', 'update', '--label-rm', label, target_hostname])

def docker_swarm_get_services_names(ip):
    service_names, _ = run_remote_command(ip, ['docker', 'service', 'ls', '--format', '"{{.Name}}"'])
    return service_names.split('\n')

def docker_swarm_get_service_id_by_name(ip, hostname):
    service_id, _ = run_remote_command(ip, ['docker', 'service', 'ls', '-f', 'name={}'.format(hostname), '--format', '"{{.ID}}"'])
    return service_id.strip()

def docker_swarm_get_service_replicas(ip, service_fullname):
    replicas_info, _ = run_remote_command(ip, ['docker', 'service', 'ls', '-f', 'name={}'.format(service_fullname), '--format', '"{{.Replicas}}"'])
    replicas_info = replicas_info.strip()
    if len(replicas_info) < 3:
        raise Exception("The replicas info cannot be parsed. Message is {}".format(replicas_info))
    return int(replicas_info[0]), int(replicas_info[2]) # a bit dirty

def docker_swarm_get_service_current_replicas(ip, service_fullname):
    return docker_swarm_get_service_replicas(ip, service_fullname)[0]

def docker_swarm_get_service_max_replicas(ip, service_fullname):
    return docker_swarm_get_service_replicas(ip, service_fullname)[1]

def docker_swarm_scale_service(ip, service_id, replicas):
    return run_remote_command(ip, ['docker', 'service', 'scale', '{}={}'.format(service_id, replicas)])

def docker_swarm_scale_service_set(ip, service_ids, replicas):
    services_to_scale = []
    for service_id in service_ids:
        services_to_scale.append('{}={}'.format(service_id, replicas))
    return run_remote_command(ip, ['docker', 'service', 'scale'].extend(services_to_scale))

def docker_swarm_get_task_name_by_hostname(ip, service_fullname, hostname):
    task_name, _ = run_remote_command(ip, ['docker', 'service', 'ps', service_fullname, '-f', 'node={}'.format(hostname), '-f', 'desired-state=running', '--format', '"{{.Name}}"'])
    return task_name.strip()

def get_docker_swarm_manager_properties(machine_config):
    for machine in machine_config['machines'].values():
        if machine['role'] == 'manager':
            return machine
    raise Exception

def get_docker_swarm_worker_properties(machine_config, hostname):
    return machine_config['machines'][hostname]

def get_services_by_placement_label(swarm_config, label):
    services = set()
    for service, properties in swarm_config['services'].items():
        if 'placement_label' in properties.keys():
            if properties['placement_label'] == label:
                services.add(service)
    return services

def get_placement_label_from_service(swarm_config, service):
    if 'placement_label' in swarm_config['services'][service].keys():
        return swarm_config['services'][service]['placement_label']
    raise Exception('Service {} has no placement label'.format(service))

def get_engine_grace_period(compose_config):
    for value in compose_config['services']['boki-engine']['entrypoint']:
        if '--scale_in_grace_period' in value:
            key_value = value.split('=')
            return key_value[1].strip()
    raise Exception('No engine grace period defined')

def get_shared_log_node_id(ip, hostname, service_fullname):
    # convention: task is foo.X where X is a number
    task_name = docker_swarm_get_task_name_by_hostname(ip, service_fullname, hostname)
    if task_name.count('.') != 1:
        raise Exception('Bad task naming')
    return(int(task_name.split('.')[1]))

def load_machine_config(base_dir):
    machine_config_file = 'machines.json'
    if not os.path.exists(os.path.join(base_dir, machine_config_file)):
        raise Exception('Could not find file {}!'.format(machine_config_file))
    with open(os.path.join(base_dir, machine_config_file)) as f:
        machine_config = json.load(f)
    return machine_config

def load_swarm_config(base_dir):
    swarm_config_file = 'config.json'
    if not os.path.exists(os.path.join(base_dir, swarm_config_file)):
        raise Exception('Could not find file {}!'.format(swarm_config_file))
    with open(os.path.join(base_dir, swarm_config_file)) as f:
        swarm_config = json.load(f)
    return swarm_config

def load_compose_config(base_dir):
    compose_config_file = 'docker-compose.yml'
    if not os.path.exists(os.path.join(base_dir, compose_config_file)):
        raise Exception('Could not find file {}!'.format(compose_config_file))
    with open(os.path.join(base_dir, compose_config_file)) as f:
        compose_config = yaml.safe_load(f)
    return compose_config

def get_manager_ip(base_dir):
    machine_config = load_machine_config(base_dir)
    manager_prop = get_docker_swarm_manager_properties(machine_config)
    return manager_prop['ip']

def node_join_script(base_dir, hostname):
    print('{} will join the swarm'.format(hostname))
    machine_config = load_machine_config(base_dir)
    manager_prop = get_docker_swarm_manager_properties(machine_config)
    worker_prop = get_docker_swarm_worker_properties(machine_config, hostname)
    manager_ip = manager_prop['ip']
    worker_ip = worker_prop['ip']
    swarm_token = docker_swarm_get_worker_token(manager_ip)
    docker_swarm_join(worker_ip, swarm_token, manager_ip)
    docker_swarm_add_label(manager_ip, hostname, worker_prop['labels'][0])
    print('{} has joint the swarm'.format(hostname))

def node_leave_script(base_dir, hostname):
    print('{} will leave the swarm'.format(hostname))
    machine_config = load_machine_config(base_dir)
    manager_prop = get_docker_swarm_manager_properties(machine_config)
    worker_prop = get_docker_swarm_worker_properties(machine_config, hostname)
    manager_ip = manager_prop['ip']
    worker_ip = worker_prop['ip']
    if hostname not in docker_swarm_get_nodes_hostnames(manager_ip):
        raise Exception('Hostname {} does not exist'.format(hostname))
    docker_swarm_leave(worker_ip)
    time.sleep(20)
    worker_node = docker_swarm_get_node_id_by_hostname(manager_ip, hostname)
    docker_swarm_remove_node(manager_ip, worker_node)
    print('{} has left the swarm and its node {} was removed'.format(hostname, worker_node))

def node_rejoin_script(base_dir, hostname):
    node_leave_script(base_dir, hostname)
    time.sleep(5)
    node_join_script(base_dir, hostname)

def service_scale_script(base_dir, main_service, replicas, service_has_dependencies = True):
    machine_config = load_machine_config(base_dir)
    manager_prop = get_docker_swarm_manager_properties(machine_config)
    manager_ip = manager_prop['ip']
    dependent_services = set()
    if service_has_dependencies:
        swarm_config = load_swarm_config(base_dir)
        placement_label = get_placement_label_from_service(swarm_config, main_service)
        dependent_services.update(get_services_by_placement_label(swarm_config, placement_label))
        dependent_services.remove(main_service)
    service_full_name = 'boki-experiment_' + main_service
    if service_full_name not in docker_swarm_get_services_names(manager_ip):
        raise Exception("Service {} does not exist".format(service_full_name))
    service_id = docker_swarm_get_service_id_by_name(manager_ip, service_full_name)
    docker_swarm_scale_service(manager_ip, service_id, replicas)
    print('Manager has scaled main service {} to {} replicas'.format(main_service, replicas))
    time.sleep(3)
    dependent_services_ids = set()
    for service in dependent_services:
        service_full_name = 'boki-experiment_' + service
        service_id = docker_swarm_get_service_id_by_name(manager_ip, service_full_name)
        dependent_services_ids.add(service_id)
    docker_swarm_scale_service_set(manager_ip, dependent_services_ids, replicas)
    print('Manager has scaled dependent services {} to {} replicas'.format(' '.join(dependent_services), replicas))

def service_scale_out_script(base_dir, joining_nodes_hostnames, service, replicas, service_has_dependencies = True):
    full_service_name = 'boki-experiment_' + service
    current_replicas = docker_swarm_get_service_current_replicas(get_manager_ip(base_dir), full_service_name)
    if replicas <= current_replicas:
        raise Exception('Cannot scale out because desired replicas {} not higher than current replicas {}'.format(replicas, current_replicas))
    for hostname in joining_nodes_hostnames:
        node_join_script(base_dir, hostname)
        time.sleep(5)
    service_scale_script(base_dir, service, replicas, service_has_dependencies)

def service_scale_in_script(base_dir, leaving_nodes_hostnames, service, replicas, service_has_dependencies = True):
    service_full_name = 'boki-experiment_' + service
    current_replicas = docker_swarm_get_service_current_replicas(get_manager_ip(base_dir), service_full_name)
    if (current_replicas - replicas) is not len(leaving_nodes_hostnames):
        raise Exception('Cannot scale in because leaving nodes not same as removing replicas')
    for hostname in leaving_nodes_hostnames:
        node_leave_script(base_dir, hostname)
    service_scale_script(base_dir, service, replicas, service_has_dependencies)

def boki_engine_scale_out_script(base_dir, engine_hostnames, engine_service, replicas, service_has_dependencies = True):
    print('Service {} of boki engines will be scaled out to {} replicas'.format(engine_service, replicas))
    service_scale_out_script(base_dir, engine_hostnames, engine_service, replicas, service_has_dependencies)

def boki_engine_scale_in_script(base_dir, engine_hostname, engine_service, zk_client, zk_server_host, zk_server_port):
    print('Service {} of boki engines will be scaled in by removing 1 replica'.format(engine_service))
    compose_config = load_compose_config(base_dir)
    service_full_name = 'boki-experiment_' + engine_service
    grace_period = int(get_engine_grace_period(compose_config))
    slog_node_id = get_shared_log_node_id(get_manager_ip(base_dir), engine_hostname, service_full_name)
    print('Scale in engine node {}'.format(slog_node_id))
    run_command([zk_client, '-server', '{}:{}'.format(zk_server_host, str(zk_server_port)), 'create', '/faas/scale/in_engine_{}'.format(str(slog_node_id))])
    print('Grace period of engine node {} started'.format(slog_node_id))
    time.sleep(grace_period + 5)
    run_command([zk_client, '-server', '{}:{}'.format(zk_server_host, str(zk_server_port)), 'delete', '/faas/scale/in_engine_{}'.format(str(slog_node_id))])
    print('Remove log engine {} from system'.format(slog_node_id))
    current_replicas = docker_swarm_get_service_current_replicas(get_manager_ip(base_dir), service_full_name)
    service_scale_in_script(base_dir, [engine_hostname], engine_service, current_replicas - 1, True)
    
def boki_engine_continously_scale_out_script(base_dir, engine_hostnames : List[str], engine_service, duration, interval):
    service_full_name = 'boki-experiment_' + engine_service
    t_start = time.time()
    t_end = time.time()
    t_elapsed = t_end - t_start
    c = 0
    while t_elapsed < duration or 0 < len(engine_hostnames):
        time.sleep(interval)
        current_replicas = docker_swarm_get_service_current_replicas(get_manager_ip(base_dir), service_full_name)
        boki_engine_scale_out_script(base_dir, [engine_hostnames.pop()], engine_service, current_replicas+1, True)
        t_end = time.time()
        t_elapsed = t_end - t_start
        c += 1
    print('Scaling finished. Scaled out {} replicas in total'.format(c))

def boki_engine_continously_scale_in_script(base_dir, engine_hostnames : List[str], engine_service, zk_client, zk_server_host, zk_server_port, duration, interval):
    t_start = time.time()
    t_end = time.time()
    t_elapsed = t_end - t_start
    c = 0
    while t_elapsed < duration or 0 < len(engine_hostnames):
        time.sleep(interval)
        boki_engine_scale_in_script(base_dir, engine_hostnames.pop(), engine_service, zk_client, zk_server_host, zk_server_port)
        t_end = time.time()
        t_elapsed = t_end - t_start
        c += 1
    print('Scaling finished. Scaled in {} replicas in total'.format(c))

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('cmd', type=str)
    parser.add_argument('--base-dir', type=str)
    parser.add_argument('--hostname', type=str)
    parser.add_argument('--hostnames', type=str, default='')
    parser.add_argument('--service', type=str, default='')
    parser.add_argument('--replicas', type=int, default=1)
    parser.add_argument('--duration', type=int, default=60)
    parser.add_argument('--interval', type=int, default=15)
    parser.add_argument('--zk_client', type=str, default='')
    parser.add_argument('--zk_server_host', type=str, default='')
    parser.add_argument('--zk_server_port', type=int, default=2181)
    args = parser.parse_args()
    try:
        if args.cmd == 'node-join':
            node_join_script(args.base_dir, args.hostname)
        elif args.cmd == 'node-rejoin':
            node_rejoin_script(args.base_dir, args.hostname)
        elif args.cmd == 'node-leave':
            node_leave_script(args.base_dir, args.hostname)
        elif args.cmd == 'boki-engine-scale-out':
            hostnames = args.hostnames.split(',')
            boki_engine_scale_out_script(args.base_dir, hostnames, args.service, args.replicas)
        elif args.cmd == 'boki-engine-scale-in':
            boki_engine_scale_in_script(args.base_dir, args.hostname, args.service, args.zk_client, args.zk_server_host, args.zk_server_port)
        elif args.cmd == 'boki-engine-scale-out-continously':
            hostnames = args.hostnames.split(',')
            boki_engine_continously_scale_out_script(args.base_dir, hostnames, args.service, args.duration, args.interval)
        elif args.cmd == 'boki-engine-scale-in-continously':
            hostnames = args.hostnames.split(',')
            boki_engine_continously_scale_in_script(args.base_dir, hostnames, args.service, args.zk_client, args.zk_server_host, args.zk_server_port, args.duration, args.interval)
        else:
            raise Exception('Unknown command: ' + args.cmd)
    except Exception as e:
        err_str = str(e)
        if not err_str.endswith('\n'):
            err_str += '\n'
        sys.stderr.write(err_str)
        sys.exit(1)
