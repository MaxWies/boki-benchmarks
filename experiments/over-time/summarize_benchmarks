#!/usr/bin/python3
from datetime import datetime, time
from operator import itemgetter
import os
import sys
import json
import argparse
import subprocess as sp
import matplotlib.pyplot as plt
import matplotlib.offsetbox as offsetbox
import numpy as np
import math
import pandas as pd
import csv

def discard_csv_files(directory, ts, before = True):
    csv_file_names = [file for file in os.listdir(directory) if file.endswith('.csv')]
    csv_files_to_discard = []
    for csv_file_name in csv_file_names:
        chunks = csv_file_name.split('.')[0].split('-')
        ts_file = int(chunks[3])
        if before:
            if ts_file < ts:
                csv_files_to_discard.append(os.path.join(directory, csv_file_name))
        else:
            if ts < ts_file:
                csv_files_to_discard.append(os.path.join(directory, csv_file_name))
    s = "before"
    if not before:
        s = "after"
    print("Remove {} files because they are {} {}".format(len(csv_files_to_discard), s, str(ts)))
    for csv_file in csv_files_to_discard:
        os.remove(csv_file)

def discard_csv_entries(csv_file, ts, before = True):
    df = pd.read_csv(csv_file)
    if before:
        df.drop(df[df.ts_absolute < ts].index, inplace=True)
    else:
        df.drop(df[ts < df.ts_absolute].index, inplace=True)
    df.to_csv((csv_file), mode='w', sep=',', index=False, header=True)

def merge_csv_files(directory, filter, result_file):
    csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and filter in file]
    print("Merge {}".format(', '.join(csv_files)))
    combined_csv = pd.concat([pd.read_csv(os.path.join(directory, file)) for file in csv_files])
    combined_csv.to_csv(result_file, index=False)

def concatenate(directory, csv_file_names, filter, result_file):
    csv_files = [os.path.join(directory, file) for file in csv_file_names if filter in file]
    with open(result_file, 'w') as f:
        for csv_file in csv_files:
            with open(csv_file) as f_in:
                for line in f_in:
                    f.write(line)

def count_csv_entries(csv_files):
    return pd.concat([pd.read_csv(file) for file in csv_files]).shape[0]

def count_csv_entries_in_directory(directory, filter):
    csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and filter in file]
    print("Count entries of {}".format(', '.join(csv_files)))
    return pd.concat([pd.read_csv(os.path.join(directory, file)) for file in csv_files]).shape[0]

def compute_csv_percentile(csv_file, column_ix, percentile):
    return pd.read_csv(csv_file).iloc[:, column_ix].quantile(q=percentile)

def add_row(directory, latency_append_file, slog, interval, result_file):
    # Get the corrsponding csv files for reads and index memory
    assert 'append' in latency_append_file
    latency_append_file_name = os.path.basename(latency_append_file)
    name_chunks = latency_append_file_name.split('.')[0].split('-') # latencies-append-NODE_ID-TS.csv
    latency_read_file = os.path.join(directory, "latencies-read-{}-{}.csv".format(name_chunks[2], name_chunks[3]))
    index_memory_file = os.path.join(directory, "index-memory-{}-{}.csv".format(name_chunks[2], name_chunks[3]))
    if not os.path.isfile(latency_read_file):
        raise Exception("csv file {} not exists".format(latency_read_file))
    if not os.path.isfile(index_memory_file):
        raise Exception("csv file {} not exists".format(index_memory_file))
    absolute_ts = int(name_chunks[3])
    index_memory = 0
    with open(index_memory_file, newline='') as f:
        index_memory = int(next(csv.reader(f))[0]) 
    # Write to result file
    write_header = not os.path.exists(result_file)
    print("Add row for slog={}, ts={}".format(slog, absolute_ts))
    with open(result_file, 'a', encoding='UTF8', newline='') as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(['slog', 'ts_absolute', 'throughput', 'throughput_append', 'throughput_read', 'latency_append_50','latency_append_99', 'latency_read_50','latency_read_99','index_memory'])
        data = [
            slog,
            absolute_ts,
            round(count_csv_entries([latency_append_file, latency_read_file]) / interval / 1000, 2),
            round(count_csv_entries([latency_append_file]) / interval / 1000, 2),
            round(count_csv_entries([latency_read_file]) / interval / 1000, 2),
            int(compute_csv_percentile(latency_append_file, 0, 0.5)),
            int(compute_csv_percentile(latency_append_file, 0, 0.99)),
            int(compute_csv_percentile(latency_read_file, 0, 0.5)),
            int(compute_csv_percentile(latency_read_file, 0, 0.99)),
            index_memory
        ]
        writer.writerow(data)

def add_rows(directory, slog, interval, result_file):
    latency_append_files = sorted([os.path.join(directory, csv_file_name) for csv_file_name in os.listdir(directory) if 'combined-latencies-append' in csv_file_name])
    for latency_append_file in latency_append_files:
        print(latency_append_file)
        latency_append_file_name = os.path.basename(latency_append_file)
        name_chunks = latency_append_file_name.split('.')[0].split('-') # latencies-append-NODE_ID-TS.csv
        latency_read_file = os.path.join(directory, "combined-latencies-read-{}.csv".format(name_chunks[3]))
        if not os.path.isfile(latency_read_file):
            raise Exception("csv file {} not exists".format(latency_read_file))
        absolute_ts = int(name_chunks[3])
        # Write to result file
        write_header = not os.path.exists(result_file)
        print("Add row for slog={}, ts={}".format(slog, absolute_ts))
        with open(result_file, 'a', encoding='UTF8', newline='') as f:
            writer = csv.writer(f)
            if write_header:
                writer.writerow(['slog', 'ts_absolute', 'throughput', 'latency_append_50','latency_append_99', 'latency_read_50','latency_read_99'])
            data = [
                slog,
                absolute_ts,
                round(count_csv_entries([latency_append_file, latency_read_file]) / interval / 1000, 2),
                int(compute_csv_percentile(latency_append_file, 0, 0.5)),
                int(compute_csv_percentile(latency_append_file, 0, 0.99)),
                int(compute_csv_percentile(latency_read_file, 0, 0.5)),
                int(compute_csv_percentile(latency_read_file, 0, 0.99))
            ]
            writer.writerow(data)

def combine_csv_files_filtered(directory, csv_file_names, engine_ids):
    ts_matrix = []
    num_ts_per_engine = 0
    print("Engine ids:")
    print(engine_ids)
    for id in engine_ids:
        engine_id_csv_files = [file for file in csv_file_names if '-{}-'.format(id) in file]
        ts_of_csv_files = [file.split('.')[0].split('-')[3] for file in engine_id_csv_files]
        ts_matrix.append((id, sorted(ts_of_csv_files)))
        if num_ts_per_engine != 0 and num_ts_per_engine != len(ts_of_csv_files):
            print("Warning: All engines should have the same number of csv files")
        num_ts_per_engine = max(num_ts_per_engine, len(ts_of_csv_files))

    for i in range(num_ts_per_engine):
        related_csv_file_names = []
        for ts_row in ts_matrix:
            id, ts_list = ts_row
            if i < len(ts_list):
                ts = ts_list[i]
                related_csv_file_names.extend([file for file in csv_file_names if '-{}-{}'.format(id, ts) in file])

        id, ts_list = ts_matrix[0]
        ts = ts_list[i]
        for op in ["append", "read"]:
            combined_result_file = os.path.join(directory, "combined-latencies-{}-{}.csv".format(op, ts))
            # now we concatenate the csv files that belong together
            concatenate(directory, related_csv_file_names, op, combined_result_file)

def combine_csv_files(directory, slog, interval, result_file):
    csv_file_names = sorted([file for file in os.listdir(directory) if file.endswith('.csv')])
    
    engine_ids = set()
    filtered_csv_file_names = []
    for csv_file_name in csv_file_names:
        chunks = csv_file_name.split('.')[0].split('-')
        ts = int(chunks[3])
        filtered_csv_file_names.append(csv_file_name)
        engine_ids.add(int(chunks[2]))
    engine_ids = sorted(engine_ids)
    combine_csv_files_filtered(directory, filtered_csv_file_names, engine_ids)

    add_rows(directory, slog, interval, result_file)

def add_column(csv_file, loc, column_name, value):
    print('Add column {}'.format(column_name))
    df = pd.read_csv(csv_file)
    df.insert(loc, column_name, value)
    df.to_csv(csv_file, mode='w', sep=',', index=False, header=True)

def update_column_by_division(csv_file, column, divisor, round_decimals):
    df = pd.read_csv(csv_file)
    for i in df.index:
        df.at[i, column] = round(df.iloc[i][column] / divisor, round_decimals)
    df.to_csv(csv_file, mode='w', sep=',', index=False, header=True)

def make_time_relative(csv_file, reference_ts, csv_result_file):
    print('Make time relative {}'.format(csv_file))
    df = pd.read_csv(csv_file)
    df.drop(df[df.ts_absolute < reference_ts].index, inplace=True)
    relative_timestamps = []
    for i in range(len(df)):
        relative_timestamps.append(df.iloc[i]['ts_absolute'] - reference_ts)
    df.insert(2, 'ts_relative', relative_timestamps)
    write_header = not os.path.isfile(csv_result_file)
    df.to_csv(csv_result_file, mode='a', sep=',', index=False, header=write_header)

def generate_plot_time_vs_throughput(csv_file, result_file):
    df = pd.read_csv(csv_file)

    df_boki = df.loc[df['slog'] == 'boki-local']
    df_indilog = df.loc[df['slog'] == 'indilog-local']

    plt.plot(df_boki['ts_relative'], df_boki['throughput'], '-', color='royalblue', marker='x', label='Boki Throughput', linewidth='2')
    plt.plot(df_indilog['ts_relative'], df_indilog['throughput'], '-', color='darkgreen', marker='o', label='Indilog Throughput', linewidth='2')

    plt.xlabel('Time (s)', fontsize=12)
    plt.ylabel('Throughput (kOp/s)', fontsize=12)

    throughput_max = df['throughput'].max()

    plt.ylim(bottom=0, top=throughput_max+50)

    plt.legend(loc='lower right', prop={'size': 12})

    plt.savefig(result_file, bbox_inches='tight')
    plt.close

def generate_plot_time_vs_latency_append(csv_file, result_file):
    df = pd.read_csv(csv_file)

    df_boki = df.loc[df['slog'] == 'boki-local']
    df_indilog = df.loc[df['slog'] == 'indilog-local']

    plt.plot(df_boki['ts_relative'], df_boki['latency_append_50'], '--', color='royalblue', marker='x', label='Boki Append 0,5', linewidth='2')
    plt.plot(df_boki['ts_relative'], df_boki['latency_append_99'], '-', color='royalblue', marker='x', label='Boki Append 0,99', linewidth='2')

    plt.plot(df_indilog['ts_relative'], df_indilog['latency_append_50'], '--', color='darkgreen', marker='o', label='Indilog Append 0,5', linewidth='2')
    plt.plot(df_indilog['ts_relative'], df_indilog['latency_append_99'], '-', color='darkgreen', marker='o', label='Indilog Append 0,99', linewidth='2')

    plt.xlabel('Time (s)', fontsize=12)
    plt.ylabel('Latency (\u03BCs)', fontsize=12)

    plt.legend(prop={'size': 12})

    plt.savefig(result_file, bbox_inches='tight')
    plt.close

def generate_plot_time_vs_latency_read(csv_file, result_file):
    df = pd.read_csv(csv_file)

    df_boki = df.loc[df['slog'] == 'boki-local']
    df_indilog = df.loc[df['slog'] == 'indilog-local']

    plt.plot(df_boki['ts_relative'], df_boki['latency_read_50'], '--', color='royalblue', marker='x', label='Boki Read 0,5', linewidth='2')
    plt.plot(df_boki['ts_relative'], df_boki['latency_read_99'], '-', color='royalblue', marker='x', label='Boki Read 0,99', linewidth='2')

    plt.plot(df_indilog['ts_relative'], df_indilog['latency_read_50'], '--', color='darkgreen', marker='o', label='Indilog Read 0,5', linewidth='2')
    plt.plot(df_indilog['ts_relative'], df_indilog['latency_read_99'], '-', color='darkgreen', marker='o', label='Indilog Read 0,99', linewidth='2')

    plt.xlabel('Time (s)', fontsize=12)
    plt.ylabel('Latency (\u03BCs)', fontsize=12)

    plt.legend(prop={'size': 12})

    plt.savefig(result_file, bbox_inches='tight')
    plt.close

def generate_plot_time_vs_memory(index_memory_csv_file, cpu_memory_csv_file, result_file):
    df_index_memory = pd.read_csv(index_memory_csv_file)
    df_ram_memory = pd.read_csv(cpu_memory_csv_file)

    df_index_memory_boki = df_index_memory.loc[df_index_memory['slog'] == 'boki-local']
    df_index_memory_indilog = df_index_memory.loc[df_index_memory['slog'] == 'indilog-local']

    df_ram_memory_boki = df_ram_memory.loc[df_ram_memory['slog'] == 'boki-local']
    df_ram_memory_indilog = df_ram_memory.loc[df_ram_memory['slog'] == 'indilog-local']

    plt.plot(df_index_memory_boki['ts_relative'], df_index_memory_boki['index_memory'], linestyle='dashed', color='royalblue', label='Boki Index Memory', linewidth='2')
    plt.plot(df_ram_memory_boki['ts_relative'], df_ram_memory_boki['mem_tot'], linestyle='solid', color='royalblue', label='Boki RAM of VM', linewidth='2')

    plt.plot(df_index_memory_indilog['ts_relative'], df_index_memory_indilog['index_memory'], linestyle='dotted', color='darkgreen', label='Indilog Index Memory', linewidth='3')
    plt.plot(df_ram_memory_indilog['ts_relative'], df_ram_memory_indilog['mem_tot'], linestyle='dashdot', color='darkgreen', label='Indilog RAM of VM', linewidth='2')

    plt.xlabel('Time (s)', fontsize=12)
    plt.ylabel('Memory (MB)', fontsize=12)

    plt.ylim(bottom=0)

    plt.legend(prop={'size': 12})

    plt.savefig(result_file, bbox_inches='tight')
    plt.close

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('cmd', type=str)
    parser.add_argument('--directory', type=str)
    parser.add_argument('--file', type=str)
    parser.add_argument('--files', type=str)
    parser.add_argument('--slog', type=str)
    parser.add_argument('--exp-duration', type=int)
    parser.add_argument('--interval', type=int)
    parser.add_argument('--reference-ts', type=int)
    parser.add_argument('--ts', type=int)
    parser.add_argument('--filter', type=str)
    parser.add_argument('--column', type=str)
    parser.add_argument('--divisor', type=int)
    parser.add_argument('--round-decimals', type=int)
    parser.add_argument('--result-file', type=str)
    parser.add_argument('--result-directory', type=str)
    args = parser.parse_args()
    try:
        if args.cmd == 'discard-csv-files-before':
            discard_csv_files(args.directory, args.ts, before=True)
        elif args.cmd == 'discard-csv-files-after':
            discard_csv_files(args.directory, args.ts, before=False) # == after
        elif args.cmd == 'discard-csv-entries-before':
            discard_csv_entries(args.file, args.ts, before=True)
        elif args.cmd == 'discard-csv-entries-after':
            discard_csv_entries(args.file, args.ts, before=False) # == after
        elif args.cmd == 'merge-csv':
            merge_csv_files(args.directory, args.filter, args.result_file)
        elif args.cmd == 'add-row':
            add_row(args.directory, args.file, args.slog, args.interval, args.result_file)
        elif args.cmd == 'add-slog-info':
            add_column(args.file, 0, 'slog', args.slog)
        elif args.cmd == 'update-column-by-division':
            update_column_by_division(args.file, args.column, args.divisor, args.round_decimals)
        elif args.cmd == 'combine-csv-files':
            combine_csv_files(args.directory, args.slog, args.interval, args.result_file)
        elif args.cmd == 'make-time-relative':
            make_time_relative(args.file, args.reference_ts, args.result_file)
        elif args.cmd == 'generate-plot-time-vs-latency-append':
            generate_plot_time_vs_latency_append(args.file, args.result_file)
        elif args.cmd == 'generate-plot-time-vs-latency-read':
            generate_plot_time_vs_latency_read(args.file, args.result_file)
        elif args.cmd == 'generate-plot-time-vs-throughput':
            generate_plot_time_vs_throughput(args.file, args.result_file)
        elif args.cmd == 'generate-plot-time-vs-memory':
            files = args.files.split(',')
            generate_plot_time_vs_memory(files[0], files[1], args.result_file)
        else:
            raise Exception('Unknown command: ' + args.cmd)
    except Exception as e:
        err_str = str(e)
        if not err_str.endswith('\n'):
            err_str += '\n'
        sys.stderr.write(err_str)
        sys.exit(1)